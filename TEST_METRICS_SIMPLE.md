# ğŸ“Š Unit Test Results in Grafana - Quick Start Guide

## Quy TrÃ¬nh LÃ m Viá»‡c

```
Cháº¡y Tests â†’ Generate JUnit XML â†’ Parse XML â†’ Prometheus Metrics â†’ Grafana Dashboard
```

## CÃ¡ch Sá»­ Dá»¥ng

### **1ï¸âƒ£ Cháº¡y Tests vÃ  Export Metrics**

**Option A: Cháº¡y táº¥t cáº£ services**
```bash
cd d:\cnpm\CNPM-3
bash scripts/run-tests-and-export.sh
```

**Option B: Cháº¡y má»™t service**
```bash
cd d:\cnpm\CNPM-3\DoAnCNPM_Backend\user_service
mvn clean test
cd ../..
python3 scripts/parse-junit-to-prometheus.py
```

### **2ï¸âƒ£ Khá»Ÿi Ä‘á»™ng Infrastructure**

```bash
# Terminal 1: Khá»Ÿi Ä‘á»™ng Prometheus, Grafana, etc.
docker-compose up -d

# Terminal 2: Khá»Ÿi Ä‘á»™ng metrics server
python3 scripts/serve-metrics.py
```

### **3ï¸âƒ£ Xem Results trong Grafana**

1. Truy cáº­p: http://localhost:3001
2. ÄÄƒng nháº­p: `admin` / `1admin1`
3. Má»Ÿ dashboard: **Unit Test Results**

## ğŸ“ˆ CÃ¡c Metrics Hiá»ƒn Thá»‹

| Metric | Ã NghÄ©a |
|--------|---------|
| `unit_tests_total` | Tá»•ng sá»‘ tests |
| `unit_tests_passed` | Sá»‘ tests pass âœ… |
| `unit_tests_failed` | Sá»‘ tests fail âŒ |
| `unit_tests_errors` | Sá»‘ lá»—i ğŸ”´ |
| `unit_tests_skipped` | Sá»‘ tests skip â­ï¸ |
| `unit_tests_duration_seconds` | Thá»i gian cháº¡y â±ï¸ |
| `unit_tests_success_rate` | % thÃ nh cÃ´ng ğŸ“Š |

## ğŸ—ï¸ Tá»‡p ÄÆ°á»£c Táº¡o

```
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ parse-junit-to-prometheus.py    # Parse JUnit XML â†’ Prometheus metrics
â”‚   â”œâ”€â”€ serve-metrics.py                # HTTP server expose metrics (port 9091)
â”‚   â””â”€â”€ run-tests-and-export.sh         # Wrapper: cháº¡y tests + export
â”œâ”€â”€ monitoring/
â”‚   â”œâ”€â”€ metrics/
â”‚   â”‚   â””â”€â”€ unit-tests.prom             # Metrics file (tá»± Ä‘á»™ng táº¡o)
â”‚   â””â”€â”€ prometheus.yml                  # Updated vá»›i unit-tests job
â””â”€â”€ monitoring/grafana/provisioning/dashboards/
    â””â”€â”€ unit-tests-dashboard.json       # Dashboard 6 panels
```

## ğŸ”§ Workflow Chi Tiáº¿t

### **Step 1: Parse Test Results**
```python
# parse-junit-to-prometheus.py
- TÃ¬m táº¥t cáº£ JUnit XML files: target/surefire-reports/*.xml
- Extract: tests, failures, errors, skipped, duration
- TÃ­nh: success rate = (passed/total) * 100
- Output: monitoring/metrics/unit-tests.prom
```

### **Step 2: Expose Metrics**
```python
# serve-metrics.py trÃªn port 9091
- GET /metrics â†’ Ä‘á»c unit-tests.prom
- Format: Prometheus text format
- Prometheus scrape: http://localhost:9091/metrics má»—i 30s
```

### **Step 3: Prometheus Query**
```
# prometheus.yml
job_name: "unit-tests"
  targets: ["localhost:9091"]
  scrape_interval: 30s
```

### **Step 4: Grafana Visualization**
```json
// unit-tests-dashboard.json
6 panels:
1. Tests Passed (line chart)
2. Tests Failed (stat)
3. Success Rate % (gauge)
4. Execution Time (bar chart)
5. Total Tests (stacked bars)
6. Test Errors (line chart)
```

## ğŸ“ Example Output

**Sau khi cháº¡y tests:**
```
âœ“ Parsed user_service: 8/8 passed
âœ“ Parsed product_service: 5/5 passed
âœ“ Parsed order_service: 6/6 passed
âœ“ Parsed payment_service: 4/4 passed
âœ“ Metrics written to monitoring/metrics/unit-tests.prom
```

**Ná»™i dung unit-tests.prom:**
```
# HELP unit_tests_total Total number of unit tests
# TYPE unit_tests_total gauge
unit_tests_total{service="user_service"} 8 1700000000000

# HELP unit_tests_passed Number of passed tests
# TYPE unit_tests_passed gauge
unit_tests_passed{service="user_service"} 8 1700000000000

# HELP unit_tests_success_rate Test success rate percentage
# TYPE unit_tests_success_rate gauge
unit_tests_success_rate{service="user_service"} 100.0 1700000000000
```

## âœ… Checklist

- [ ] Tests cháº¡y thÃ nh cÃ´ng (JUnit XML generated)
- [ ] Parse script cháº¡y: `python3 scripts/parse-junit-to-prometheus.py`
- [ ] Metrics file táº¡o: `monitoring/metrics/unit-tests.prom`
- [ ] Metrics server khá»Ÿi Ä‘á»™ng: `python3 scripts/serve-metrics.py`
- [ ] Prometheus scrape thÃ nh cÃ´ng (check http://localhost:9090)
- [ ] Grafana dashboard hiá»ƒn thá»‹ (http://localhost:3001)

## ğŸ› Troubleshooting

**Q: Metrics khÃ´ng hiá»ƒn thá»‹ trong Grafana?**
- A: Check Prometheus targets: http://localhost:9090/targets
- Ensure metrics-server running: `docker ps | grep metrics-server`
- Check metrics file exists: `cat monitoring/metrics/unit-tests.prom`

**Q: Metrics file rá»—ng?**
- A: Check JUnit XML files generated: `find . -name "*.xml" -path "*surefire*"`
- Run parse script with debug: `python3 scripts/parse-junit-to-prometheus.py`

**Q: Port 9091 Ä‘ang sá»­ dá»¥ng?**
- A: Kill process: `netstat -ano | findstr 9091` (Windows)
- Hoáº·c change port trong serve-metrics.py

## ğŸš€ Tá»± Ä‘á»™ng hÃ³a (Optional)

ThÃªm vÃ o CI/CD pipeline (.github/workflows/ci-cd.yml):
```yaml
- name: Run tests and export metrics
  run: bash scripts/run-tests-and-export.sh
```

## ğŸ“š Tham Kháº£o

- [Prometheus Text Format](https://prometheus.io/docs/instrumenting/exposition_formats/)
- [Grafana Dashboard JSON Model](https://grafana.com/docs/grafana/latest/dashboards/)
- [Maven Surefire Report Plugin](https://maven.apache.org/surefire/maven-surefire-plugin/)
